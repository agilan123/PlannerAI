{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agilan123/PlannerAI/blob/main/AI_Scheduler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix9I-0dvf8lf",
        "outputId": "65d82e3c-2f82-4cab-8b25-4c0107eb0797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_17 (Embedding)    (None, 22, 16)            160000    \n",
            "                                                                 \n",
            " global_average_pooling1d_1  (None, 16)                0         \n",
            " 1 (GlobalAveragePooling1D)                                      \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 24)                408       \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 4)                 100       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160508 (626.98 KB)\n",
            "Trainable params: 160508 (626.98 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "175/175 [==============================] - 1s 4ms/step - loss: 1.2082 - accuracy: 0.4273 - val_loss: 0.9835 - val_accuracy: 0.6629\n",
            "Epoch 2/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.7674 - accuracy: 0.7354 - val_loss: 0.5744 - val_accuracy: 0.7657\n",
            "Epoch 3/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.7704 - val_loss: 0.4754 - val_accuracy: 0.7764\n",
            "Epoch 4/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.7898 - val_loss: 0.4278 - val_accuracy: 0.8064\n",
            "Epoch 5/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.4134 - accuracy: 0.8263 - val_loss: 0.3898 - val_accuracy: 0.8229\n",
            "Epoch 6/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.3725 - accuracy: 0.8534 - val_loss: 0.3610 - val_accuracy: 0.8664\n",
            "Epoch 7/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.8689 - val_loss: 0.3327 - val_accuracy: 0.8643\n",
            "Epoch 8/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8764 - val_loss: 0.3111 - val_accuracy: 0.8779\n",
            "Epoch 9/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.2869 - accuracy: 0.8861 - val_loss: 0.2947 - val_accuracy: 0.8771\n",
            "Epoch 10/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.2689 - accuracy: 0.8959 - val_loss: 0.2800 - val_accuracy: 0.8821\n",
            "44/44 - 0s - loss: 0.2800 - accuracy: 0.8821 - 54ms/epoch - 1ms/step\n",
            "\n",
            "Test accuracy: 0.8821428418159485\n",
            "44/44 [==============================] - 0s 939us/step\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "import sys\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('Shuffled_ML_training_scheduler.csv')\n",
        "queries = df['query'].values\n",
        "priorities = df['priority'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(queries, priorities, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_length = max(len(x) for x in X_train_seq)\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=16, input_length=max_length),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "history = model.fit(X_train_padded, y_train, epochs=10, validation_data=(X_test_padded, y_test))\n",
        "\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test_padded, y_test, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "\n",
        "\n",
        "predictions = model.predict(X_test_padded)\n",
        "predicted_priorities = np.argmax(predictions, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG56dt8LiBIj",
        "outputId": "68607018-0416-4097-8501-dfef54840c4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "Predicted Priority: 2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "new_query = [\"I have a doctors appointment sometime this week\"]\n",
        "sequence = tokenizer.texts_to_sequences(new_query)\n",
        "\n",
        "\n",
        "padded_sequence = pad_sequences(sequence, maxlen=100, padding='post')  # Use the same maxlen as during training\n",
        "\n",
        "\n",
        "prediction = model.predict(padded_sequence)\n",
        "\n",
        "predicted_priority = np.argmax(prediction, axis=1)\n",
        "print(f\"Predicted Priority: {predicted_priority[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMCU1VeSqOqv",
        "outputId": "2c922be4-3fb7-4000-b1e0-ad187c4f7ce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_18 (Embedding)    (None, 22, 16)            160000    \n",
            "                                                                 \n",
            " global_average_pooling1d_1  (None, 16)                0         \n",
            " 2 (GlobalAveragePooling1D)                                      \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 24)                408       \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160433 (626.69 KB)\n",
            "Trainable params: 160433 (626.69 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "175/175 [==============================] - 1s 3ms/step - loss: 3.7059 - mae: 1.5022 - val_loss: 1.1564 - val_mae: 0.8605\n",
            "Epoch 2/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.8996 - mae: 0.6807 - val_loss: 0.6634 - val_mae: 0.5164\n",
            "Epoch 3/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.5760 - mae: 0.4539 - val_loss: 0.4823 - val_mae: 0.3954\n",
            "Epoch 4/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.4085 - mae: 0.3734 - val_loss: 0.3535 - val_mae: 0.3386\n",
            "Epoch 5/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.2864 - mae: 0.3036 - val_loss: 0.2690 - val_mae: 0.2873\n",
            "Epoch 6/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.2154 - mae: 0.2459 - val_loss: 0.2155 - val_mae: 0.2463\n",
            "Epoch 7/10\n",
            "175/175 [==============================] - 1s 3ms/step - loss: 0.1804 - mae: 0.2152 - val_loss: 0.1876 - val_mae: 0.2150\n",
            "Epoch 8/10\n",
            "175/175 [==============================] - 1s 4ms/step - loss: 0.1624 - mae: 0.1972 - val_loss: 0.1759 - val_mae: 0.2077\n",
            "Epoch 9/10\n",
            "175/175 [==============================] - 0s 3ms/step - loss: 0.1534 - mae: 0.1847 - val_loss: 0.1675 - val_mae: 0.1912\n",
            "Epoch 10/10\n",
            "175/175 [==============================] - 0s 2ms/step - loss: 0.1477 - mae: 0.1801 - val_loss: 0.1637 - val_mae: 0.1807\n",
            "44/44 - 0s - loss: 0.1637 - mae: 0.1807 - 51ms/epoch - 1ms/step\n",
            "\n",
            "Test MAE: 0.1807045042514801\n",
            "44/44 [==============================] - 0s 918us/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "queries = df['query'].values\n",
        "intensities = df['intensity'].values  # Assuming 'intensity' column exists\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(queries, intensities, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "\n",
        "max_length = max(len(x) for x in X_train_seq)  # Or set a predefined max_length\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "# Adjust the model to fit the intensity prediction\n",
        "# The last Dense layer might need adjustments based on the range of your intensity values\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=16, input_length=max_length),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(1, activation='linear')  # For intensity, you might use a single neuron with linear activation\n",
        "])\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])  # Use MSE and MAE for regression tasks\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_padded, y_train, epochs=10, validation_data=(X_test_padded, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_mae = model.evaluate(X_test_padded, y_test, verbose=2)\n",
        "print('\\nTest MAE:', test_mae)\n",
        "\n",
        "# Making predictions\n",
        "predictions = model.predict(X_test_padded)\n",
        "# For intensity, you might directly use the predictions without needing to apply argmax since it's a regression task now\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdn5IQhoqdhS",
        "outputId": "790453df-c11c-4857-a350-5498e92e5eb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicted Intensity: 1.8994007110595703\n"
          ]
        }
      ],
      "source": [
        "# Assuming `tokenizer` and `model` are already defined and properly trained\n",
        "\n",
        "# Example query\n",
        "query = \"I have a doctors appointment\"\n",
        "\n",
        "# Tokenize and pad the query\n",
        "sequence = tokenizer.texts_to_sequences([query])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=100, padding='post')  # Ensure maxlen matches training setup\n",
        "\n",
        "# Predict intensity\n",
        "predicted_intensity = model.predict(padded_sequence)\n",
        "\n",
        "# Assuming intensity is a single regression output from the model\n",
        "print(f\"Predicted Intensity: {predicted_intensity[0][0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNGtca/sPXz8nI4Zp4iL/w2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
